{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "214ad96f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-29T22:42:43.305610Z",
     "iopub.status.busy": "2022-03-29T22:42:43.304368Z",
     "iopub.status.idle": "2022-03-29T22:42:45.261905Z",
     "shell.execute_reply": "2022-03-29T22:42:45.262502Z",
     "shell.execute_reply.started": "2022-03-29T22:40:29.975957Z"
    },
    "papermill": {
     "duration": 1.975856,
     "end_time": "2022-03-29T22:42:45.262821",
     "exception": false,
     "start_time": "2022-03-29T22:42:43.286965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sample_submission.csv\n",
      "./data/test.csv\n",
      "./data/train.csv\n",
      "What's up man?\n",
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n"
     ]
    }
   ],
   "source": [
    "# The following is modeled after the tutorial for the competition at: https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial/notebook\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, feature_selection, linear_model, model_selection, preprocessing, pipeline, decomposition, naive_bayes\n",
    "from nltk import tokenize, classify, download\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "dataframes = {}\n",
    "\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(dirname, filename)\n",
    "        dataframes[filename] = pd.read_csv(filepath)\n",
    "        print(filepath)\n",
    "\n",
    "sample = dataframes['sample_submission.csv']\n",
    "train = dataframes['train.csv']\n",
    "test = dataframes['test.csv']\n",
    "\n",
    "print(train[train[\"target\"] == 0][\"text\"].values[0])\n",
    "print(train[train[\"target\"] == 1][\"text\"].values[0])\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of normal tweets: 4342\n",
      "# of disaster tweets: 3271\n"
     ]
    }
   ],
   "source": [
    "# Data stats\n",
    "print('# of normal tweets:',train[train[\"target\"] == 0][\"text\"].count())\n",
    "print('# of disaster tweets:',train[train[\"target\"] == 1][\"text\"].count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "487fcda6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T22:42:45.297622Z",
     "iopub.status.busy": "2022-03-29T22:42:45.296871Z",
     "iopub.status.idle": "2022-03-29T22:42:45.298788Z",
     "shell.execute_reply": "2022-03-29T22:42:45.299366Z",
     "shell.execute_reply.started": "2022-03-29T22:40:30.026362Z"
    },
    "papermill": {
     "duration": 0.022009,
     "end_time": "2022-03-29T22:42:45.299536",
     "exception": false,
     "start_time": "2022-03-29T22:42:45.277527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get cross-validated scores\n",
    "def getScores(clf,train_vectors,train):\n",
    "    scores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16645215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T22:42:45.329481Z",
     "iopub.status.busy": "2022-03-29T22:42:45.328710Z",
     "iopub.status.idle": "2022-03-29T22:42:46.528438Z",
     "shell.execute_reply": "2022-03-29T22:42:46.529361Z",
     "shell.execute_reply.started": "2022-03-29T22:40:30.038151Z"
    },
    "papermill": {
     "duration": 1.216863,
     "end_time": "2022-03-29T22:42:46.529640",
     "exception": false,
     "start_time": "2022-03-29T22:42:45.312777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6025641  0.50168919 0.56985004 0.50781969 0.67275495]\n",
      "[0.62962963 0.55507372 0.64457332 0.59444444 0.72337043]\n",
      "[0.62969925 0.56480687 0.64860427 0.59332732 0.72684458]\n",
      "[0.6324473  0.59252669 0.64869419 0.59418932 0.73127036]\n",
      "[0.63377609 0.58480565 0.6366782  0.58790698 0.73717443]\n",
      "[0.62901655 0.67093236 0.66725198 0.62200957 0.75963905]\n"
     ]
    }
   ],
   "source": [
    "# Using a Count Vectorizer and Ridge Classifier\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "## Get counts for the data\n",
    "train_vectors = count_vectorizer.fit_transform(train[\"text\"])\n",
    "\n",
    "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
    "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
    "# i.e. that the train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test[\"text\"])\n",
    "\n",
    "## Our vectors are really big, so we want to push our model's weights\n",
    "## toward 0 without completely discounting different words - ridge regression \n",
    "## is a good way to do this.\n",
    "clf = linear_model.RidgeClassifier()\n",
    "\n",
    "print(getScores(clf,train_vectors,train))\n",
    "\n",
    "# Using TFID Vectorizer and Ridge Classifier\n",
    "tfid_vectorizer = feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "## Get counts for the data\n",
    "train_vectors = tfid_vectorizer.fit_transform(train[\"text\"])\n",
    "\n",
    "test_vectors = tfid_vectorizer.transform(test[\"text\"])\n",
    "\n",
    "clf = linear_model.RidgeClassifier()\n",
    "print(getScores(clf,train_vectors,train))\n",
    "\n",
    "# Using Hashing Vectorizer and  Ridge Classifier\n",
    "hashing_vectorizer = feature_extraction.text.HashingVectorizer()\n",
    "\n",
    "## Get counts for the data\n",
    "train_vectors = hashing_vectorizer.transform(train[\"text\"])\n",
    "\n",
    "test_vectors = hashing_vectorizer.transform(test[\"text\"])\n",
    "\n",
    "clf = linear_model.RidgeClassifier()\n",
    "print(getScores(clf,train_vectors,train))\n",
    "\n",
    "# Using TFID Vectorizer and SGD Classifier\n",
    "tfid_vectorizer = feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "## Get counts for the data\n",
    "train_vectors = tfid_vectorizer.fit_transform(train[\"text\"])\n",
    "\n",
    "test_vectors = tfid_vectorizer.transform(test[\"text\"])\n",
    "\n",
    "clf = linear_model.SGDClassifier(loss=\"log\")\n",
    "print(getScores(clf,train_vectors,train))\n",
    "\n",
    "# create a function for the tweet tokenizer from NLTK\n",
    "def tok(text):\n",
    "    tt = tokenize.TweetTokenizer()\n",
    "    return tt.tokenize(text)\n",
    "\n",
    "# Using TFID Vectorizer and SGD Classifier w/ Tweet Tokenizer\n",
    "tfid_vectorizer = feature_extraction.text.TfidfVectorizer(tokenizer=tok)\n",
    "\n",
    "## Get counts for the data\n",
    "train_vectors = tfid_vectorizer.fit_transform(train[\"text\"])\n",
    "\n",
    "test_vectors = tfid_vectorizer.transform(test[\"text\"])\n",
    "\n",
    "clf = linear_model.SGDClassifier(loss=\"log\")\n",
    "print(getScores(clf,train_vectors,train))\n",
    "\n",
    "# Using TFID Vectorizer and SGD Classifier w/ Tweet Tokenizer and SelectKBest Feature Selector\n",
    "tfid_vectorizer = feature_extraction.text.TfidfVectorizer(tokenizer=tok)\n",
    "chi2_selector = feature_selection.SelectKBest(feature_selection.chi2,k=1000)\n",
    "\n",
    "## Get counts for the data\n",
    "train_vectors = chi2_selector.fit_transform(tfid_vectorizer.fit_transform(train[\"text\"]),train[\"target\"])\n",
    "\n",
    "test_vectors = chi2_selector.transform(tfid_vectorizer.transform(test[\"text\"]))\n",
    "\n",
    "clf = linear_model.SGDClassifier(loss=\"log\")\n",
    "print(getScores(clf,train_vectors,train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/niklasz/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly identified training cases 0.574412189675555\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis baseline\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def is_positive(tweet: str) -> int:\n",
    "    # compound refers to overall positive/negative sentiment\n",
    "    scores = sia.polarity_scores(tweet)\n",
    "    #if sia.polarity_scores(tweet)[\"compound\"] > 0:\n",
    "    if scores['neg'] <= scores['pos']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Evaluating training data\n",
    "predictions =  train['text'].apply(is_positive)\n",
    "correct_ratio = np.where(train['target'] == predictions, 1, 0).sum()/train.shape[0]\n",
    "print(f'Correctly identified training cases {correct_ratio}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "208cc396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T22:42:59.776585Z",
     "iopub.status.busy": "2022-03-29T22:42:59.775854Z",
     "iopub.status.idle": "2022-03-29T22:42:59.801161Z",
     "shell.execute_reply": "2022-03-29T22:42:59.800413Z",
     "shell.execute_reply.started": "2022-03-29T22:40:37.465469Z"
    },
    "papermill": {
     "duration": 0.048769,
     "end_time": "2022-03-29T22:42:59.801315",
     "exception": false,
     "start_time": "2022-03-29T22:42:59.752546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  target\n",
      "0         0       1\n",
      "1         2       0\n",
      "2         3       1\n",
      "3         9       0\n",
      "4        11       1\n",
      "...     ...     ...\n",
      "3258  10861       1\n",
      "3259  10865       0\n",
      "3260  10868       1\n",
      "3261  10874       1\n",
      "3262  10875       0\n",
      "\n",
      "[3263 rows x 2 columns]\n",
      "         id keyword location  \\\n",
      "0         0     NaN      NaN   \n",
      "1         2     NaN      NaN   \n",
      "2         3     NaN      NaN   \n",
      "3         9     NaN      NaN   \n",
      "4        11     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "3258  10861     NaN      NaN   \n",
      "3259  10865     NaN      NaN   \n",
      "3260  10868     NaN      NaN   \n",
      "3261  10874     NaN      NaN   \n",
      "3262  10875     NaN      NaN   \n",
      "\n",
      "                                                   text  \n",
      "0                    Just happened a terrible car crash  \n",
      "1     Heard about #earthquake is different cities, s...  \n",
      "2     there is a forest fire at spot pond, geese are...  \n",
      "3              Apocalypse lighting. #Spokane #wildfires  \n",
      "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
      "...                                                 ...  \n",
      "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
      "3259  Storm in RI worse than last hurricane. My city...  \n",
      "3260  Green Line derailment in Chicago http://t.co/U...  \n",
      "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
      "3262  #CityofCalgary has activated its Municipal Eme...  \n",
      "\n",
      "[3263 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fit data using Ridge Classifier and training data\n",
    "# Set predictions in sample's target space\n",
    "clf.fit(train_vectors, train[\"target\"])\n",
    "sample[\"target\"] = clf.predict(test_vectors)\n",
    "# sample[\"text\"] = test[\"text\"]\n",
    "print(sample)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6690b522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T22:42:59.839676Z",
     "iopub.status.busy": "2022-03-29T22:42:59.838703Z",
     "iopub.status.idle": "2022-03-29T22:42:59.851331Z",
     "shell.execute_reply": "2022-03-29T22:42:59.850522Z",
     "shell.execute_reply.started": "2022-03-29T22:41:06.254649Z"
    },
    "papermill": {
     "duration": 0.03353,
     "end_time": "2022-03-29T22:42:59.851485",
     "exception": false,
     "start_time": "2022-03-29T22:42:59.817955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates a .csv file with the predictons\n",
    "sample.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.720036,
   "end_time": "2022-03-29T22:43:02.701102",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-29T22:42:32.981066",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}